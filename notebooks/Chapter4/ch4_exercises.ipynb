{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47738455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Number of parameters in feed forward and attention modules \n",
    "# Calculate and compare the number of parameters that are contained in the feed forward module \n",
    "# and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbc0c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/data-mining/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import TransformerBlock\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e120cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (norm1): LayerNorm()\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (norm2): LayerNorm()\n",
      "  (feedforward): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f531d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters for feed forward module: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "# Total # of parameters in the feed forward module of the Transformer block\n",
    "total_params = sum(p.numel() for p in block.feedforward.parameters())\n",
    "print(f\"Total number of parameters for feed forward module: {total_params :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454d6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in multi head attention module: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "# Total # of parameters in the multi-head attention module of the Transformer block\n",
    "total_params_attn = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Total number of parameters in multi head attention module: {total_params_attn :,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e994550",
   "metadata": {},
   "source": [
    "Mathematical breakdown using an embedding demension of 768\n",
    "\n",
    "Feed forward module:\n",
    "\n",
    "1st Linear layer: 768 inputs × 4×768 outputs + 4×768 bias units = 2,362,368\n",
    "2nd Linear layer: 4×768 inputs × 768 outputs + 768 bias units = 2,360,064\n",
    "Total: 1st Linear layer + 2nd Linear layer = 2,362,368 + 2,360,064 = 4,722,432\n",
    "\n",
    "Attention module:\n",
    "\n",
    "W_query: 768 inputs × 768 outputs = 589,824\n",
    "W_key: 768 inputs × 768 outputs = 589,824\n",
    "W_value: 768 inputs × 768 outputs = 589,824\n",
    "out_proj: 768 inputs × 768 outputs + 768 bias units = 590,592\n",
    "Total: W_query + W_key + W_value + out_proj = 3×589,824 + 590,592 = 2,360,064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 Initializing larger GPT models \n",
    "# We initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.”\n",
    "# Without making any code modifications besides updating the configuration file, use\n",
    "# the GPTModel class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-\n",
    "# dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads),\n",
    "# and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head\n",
    "# attention heads). As a bonus, calculate the total number of parameters in each GPT\n",
    "# model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5432b",
   "metadata": {},
   "source": [
    "GPT2-small (the 124M configuration we already implemented):\n",
    "\n",
    "\"emb_dim\" = 768\n",
    "\"n_layers\" = 12\n",
    "\"n_heads\" = 12\n",
    "\n",
    "GPT2-medium:\n",
    "\n",
    "\"emb_dim\" = 1024\n",
    "\"n_layers\" = 24\n",
    "\"n_heads\" = 16\n",
    "\n",
    "GPT2-large:\n",
    "\n",
    "\"emb_dim\" = 1280\n",
    "\"n_layers\" = 36\n",
    "\"n_heads\" = 20\n",
    "\n",
    "GPT2-XL:\n",
    "\n",
    "\"emb_dim\" = 1600\n",
    "\"n_layers\" = 48\n",
    "\"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd769e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 small configuration\n",
    "GPT_CONFIG_SMALL = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80f29049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 medium configuration\n",
    "GPT_CONFIG_MEDIUM = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 16,          # Number of attention heads\n",
    "    \"n_layers\": 24,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9517453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 large configuration\n",
    "GPT_CONFIG_LARGE = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1280, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 20,          # Number of attention heads\n",
    "    \"n_layers\": 36,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a4d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 2-XL configuration\n",
    "GPT_CONFIG_XL = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1600, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 25,          # Number of attention heads\n",
    "    \"n_layers\": 48,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34db2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_params(model):\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import GPTModel\n",
    "\n",
    "model_small = GPTModel(GPT_CONFIG_SMALL)\n",
    "calculate_params(model_small)\n",
    "\n",
    "model_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "calculate_params(model_medium)\n",
    "\n",
    "model_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "calculate_params(model_large)\n",
    "\n",
    "model_xl = GPTModel(GPT_CONFIG_XL)\n",
    "calculate_params(model_xl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
