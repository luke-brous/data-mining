{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47738455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Number of parameters in feed forward and attention modules \n",
    "# Calculate and compare the number of parameters that are contained in the feed forward module \n",
    "# and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 Initializing larger GPT models \n",
    "# We initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.”\n",
    "# Without making any code modifications besides updating the configuration file, use\n",
    "# the GPTModel class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-\n",
    "# dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads),\n",
    "# and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head\n",
    "# attention heads). As a bonus, calculate the total number of parameters in each GPT\n",
    "# model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
