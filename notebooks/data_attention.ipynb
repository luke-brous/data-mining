{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pytorch library and tiktoken module\n",
    "import torch\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696073b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the gpt2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054f7b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had given orders that day to deny my door to every one; having made a solemn resolution that morning\n"
     ]
    }
   ],
   "source": [
    "# read my short story and save it\n",
    "with open( \"../data/gautier.txt\", \"r\" ) as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text to tokens\n",
    "enc_text = tokenizer.encode_ordinary( raw_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a72812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4da0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataloader for GPTDatasetV1\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e51ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[25383,  1813,  6266,   326]]), tensor([[1813, 6266,  326, 1110]])]\n"
     ]
    }
   ],
   "source": [
    "# test the dataloader\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1813, 6266,  326, 1110]]), tensor([[6266,  326, 1110,  284]])]\n"
     ]
    }
   ],
   "source": [
    "# try another batch\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a455f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[25383,  1813]]), tensor([[1813, 6266]])]\n"
     ]
    }
   ],
   "source": [
    "# test different variables\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=2, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c59819ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[25383,  1813,  6266,   326,  1110,   284, 10129,   616]]), tensor([[ 1813,  6266,   326,  1110,   284, 10129,   616,  3420]])]\n"
     ]
    }
   ],
   "source": [
    "# test different strides and context sizes\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[25383,  1813,  6266,   326],\n",
      "        [ 1110,   284, 10129,   616],\n",
      "        [ 3420,   284,   790,   530],\n",
      "        [   26,  1719,   925,   257],\n",
      "        [26322,  6323,   326,  3329],\n",
      "        [  326,   314,   561,   466],\n",
      "        [ 2147,    11,   314,   750],\n",
      "        [  407,  4601,   284,   307]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 1813,  6266,   326,  1110],\n",
      "        [  284, 10129,   616,  3420],\n",
      "        [  284,   790,   530,    26],\n",
      "        [ 1719,   925,   257, 26322],\n",
      "        [ 6323,   326,  3329,   326],\n",
      "        [  314,   561,   466,  2147],\n",
      "        [   11,   314,   750,   407],\n",
      "        [ 4601,   284,   307, 24069]])\n"
     ]
    }
   ],
   "source": [
    "# test different batch sizes v4 but use these variables for now\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28177f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had given orders that\n",
      " day to deny my\n",
      " door to every one\n",
      "; having made a\n",
      " solemn resolution that morning\n",
      " that I would do\n",
      " nothing, I did\n",
      " not wish to be\n"
     ]
    }
   ],
   "source": [
    "# to apply the tokenizer's decoder to these IDs, the rows of the tensor `inputs` have to be converted into lists:\n",
    "for row in inputs:\n",
    "    print( tokenizer.decode( row.tolist() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.1246,  0.3535, -0.0302, -0.2198,  0.5967, -1.3539, -0.1885, -0.3454],\n",
      "        [ 1.2149, -0.4090, -0.6654,  0.4349,  0.5361,  1.0460, -0.1951,  0.7766],\n",
      "        [ 0.5441,  1.2044, -0.0768, -0.9519,  0.5792, -0.3672,  0.0118,  0.7613],\n",
      "        [-0.5808, -0.5994, -0.1550, -1.0729,  0.1593,  1.8672,  1.5700, -0.1120]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding layer\n",
    "vocab_size = 4\n",
    "output_dim = 8\n",
    "\n",
    "# A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "# This module is often used to store word embeddings and retrieve them using indices.\n",
    "# The input to the module is a list of indices, and the output is the corresponding\n",
    "# word embeddings.\n",
    "inputs = torch.nn.Embedding( vocab_size, output_dim )\n",
    "\n",
    "print( inputs.weight )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e590dfc",
   "metadata": {},
   "source": [
    "Question 9: Since LLM's cannot take in raw text, video, images, etc.. we must convert said data into numbers, and with the help of PyTorch we can turn them into numerical vectors which help the LLM process with them with their neural network operations. The embedding layer transforms the tokens we create into these numerical vectors that the model can now learn from and take in patterns from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1246,  0.3535, -0.0302, -0.2198,  0.5967, -1.3539, -0.1885, -0.3454],\n",
       "        [ 1.2149, -0.4090, -0.6654,  0.4349,  0.5361,  1.0460, -0.1951,  0.7766],\n",
       "        [ 0.5441,  1.2044, -0.0768, -0.9519,  0.5792, -0.3672,  0.0118,  0.7613],\n",
       "        [-0.5808, -0.5994, -0.1550, -1.0729,  0.1593,  1.8672,  1.5700, -0.1120]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the weights of the embedding layer\n",
    "inputs = inputs.weight.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03e85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8270,  0.3208,  3.7087, -0.6648])\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "query = inputs[2]\n",
    "attention_scores_2 = torch.zeros(len(inputs)) \n",
    "for i in range( len( inputs ) ):\n",
    "    attention_scores_2[i] = torch.dot( query, inputs[i] )\n",
    "print( attention_scores_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e30a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1271, 0.0282, 0.8342, 0.0105])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e688ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08ab41f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6250,  1.0318, -0.0883, -0.8211,  0.5758, -0.4293, -0.0031,  0.6119])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros( query.shape )\n",
    "for i in range( len( attention_weights_2 ) ):\n",
    "    context_vector_2 += attention_weights_2[i] * inputs[i]\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76a71c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7829, -0.1816,  1.8270, -3.3147],\n",
       "        [-0.1816,  4.2978,  0.3208,  0.8214],\n",
       "        [ 1.8270,  0.3208,  3.7087, -0.6648],\n",
       "        [-3.3147,  0.8214, -0.6648,  7.8611]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the attention scores via matrix multiplication\n",
    "attention_scores_2 = inputs @ inputs.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d02a1194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.6115e-01, 1.6343e-02, 1.2180e-01, 7.1227e-04],\n",
       "        [1.0688e-02, 9.4251e-01, 1.7664e-02, 2.9139e-02],\n",
       "        [1.2707e-01, 2.8179e-02, 8.3423e-01, 1.0517e-02],\n",
       "        [1.3995e-05, 8.7540e-04, 1.9805e-04, 9.9891e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores_2, dim=-1)\n",
    "attention_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "428a71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8a26841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0542,  0.4440, -0.0463, -0.2989,  0.5933, -1.1922, -0.1629, -0.1921],\n",
       "        [ 1.1498, -0.3779, -0.6333,  0.3595,  0.5265,  1.0193, -0.1399,  0.7384],\n",
       "        [ 0.6250,  1.0318, -0.0883, -0.8211,  0.5758, -0.4293, -0.0031,  0.6119],\n",
       "        [-0.5789, -0.5988, -0.1554, -1.0716,  0.1598,  1.8660,  1.5681, -0.1110]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = attention_weights @ inputs\n",
    "context_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
