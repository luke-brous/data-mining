{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84c0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates advanced attention mechanisms in PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccce9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.nn.Embedding( 4, 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d327bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01, -6.4746e-01,\n",
       "          9.2566e-01,  1.6874e+00,  1.9702e+00],\n",
       "        [ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01, -1.4050e+00,\n",
       "         -3.5821e-01, -4.6841e-01,  3.9729e-01],\n",
       "        [-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02, -1.1910e-01,\n",
       "         -5.4830e-01, -8.6023e-01, -3.4981e-01],\n",
       "        [ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00,  1.6816e+00,\n",
       "          1.2896e+00, -2.1932e-01,  1.0227e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee4840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01, -6.4746e-01,\n",
       "          9.2566e-01,  1.6874e+00,  1.9702e+00],\n",
       "        [ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01, -1.4050e+00,\n",
       "         -3.5821e-01, -4.6841e-01,  3.9729e-01],\n",
       "        [-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02, -1.1910e-01,\n",
       "         -5.4830e-01, -8.6023e-01, -3.4981e-01],\n",
       "        [ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00,  1.6816e+00,\n",
       "          1.2896e+00, -2.1932e-01,  1.0227e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9bd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimensions\n",
    "d_in = 8\n",
    "d_out = 6\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_k = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_v = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee23407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2601,  0.9193, -0.2638, -1.5932,  1.5702, -1.0805],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an input vector and transform it into our query vector using W_q\n",
    "query = inputs[2] @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0350419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[ 2.5200, -1.7649, -3.4682, -1.3480, -1.7786, -1.6056],\n",
      "        [ 2.0886, -0.3383,  0.8250,  1.9821,  0.5763, -3.3585],\n",
      "        [ 0.1160, -0.5166,  3.1862, -0.1417,  2.0463, -0.6900],\n",
      "        [-8.3785,  1.5520, -3.3874,  1.6451, -0.6418,  1.7743]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Values: tensor([[ 1.7218,  2.5810, -0.5992, -0.2433,  3.0077,  5.4760],\n",
      "        [ 4.1684,  1.2448, -0.6707, -1.1014, -2.2304, -1.5972],\n",
      "        [-1.9564, -0.1963, -1.0316, -0.7024, -2.3706,  0.8555],\n",
      "        [-6.2996,  0.2400, -0.2250, -0.8479,  0.7917,  0.8526]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\" , keys)\n",
    "print(\"Values:\" , values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.5575,   3.4793,   3.0153, -13.7833], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T # query is 1 by 6 and keys is 4 by 6 so we need to transpose keys\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6091e-01, 3.4955e-01, 2.8924e-01, 3.0400e-04],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim=-1 ) # the softmax function normalizes the scores\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a97eedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum() # ensure the weights sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f663f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5107,  1.3099, -0.7492, -0.6762, -0.3796,  1.6657],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ values # \n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6676af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_k = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_v = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7db99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac967da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2213, -0.9298, -0.5280,  0.4143, -0.5717, -0.5802],\n",
       "        [-1.3247,  1.7368, -0.3850, -0.2259,  1.8350,  2.0955],\n",
       "        [ 0.8309,  1.1852,  0.4297, -1.5593, -0.3377, -0.8801],\n",
       "        [-1.3090,  0.1422, -1.1954,  0.7969, -0.0797, -0.6121],\n",
       "        [ 0.4527, -0.6632,  1.1933, -1.9665, -0.2963, -0.8102],\n",
       "        [ 0.0694,  0.7240,  0.0411,  0.2219, -0.7028,  1.3820],\n",
       "        [ 0.8008,  0.2697, -0.0496, -0.4712,  0.5931, -1.1528],\n",
       "        [ 1.7258,  0.0110,  2.1158,  1.8705,  2.3299, -0.6982]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb72a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9613e-01,  2.2957e+00, -5.8743e-01,  6.9857e-01, -2.4537e+00,\n",
       "          1.6774e+00],\n",
       "        [-4.0592e+00,  2.5668e+00,  7.7100e-01, -4.3102e+00,  7.9865e+00,\n",
       "         -7.3634e-01],\n",
       "        [-3.7524e+00,  2.4404e+00,  8.6552e-01, -3.9805e+00,  7.3841e+00,\n",
       "         -6.1145e-01],\n",
       "        [ 5.1731e-01, -1.3085e+00, -4.3256e-03,  2.0008e+00, -4.6349e+00,\n",
       "         -9.1959e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f76a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second version of the class\n",
    "# it uses nn.Linear to do things more effectively\n",
    "\n",
    "class SimpleAttentionv2( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttentionv2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e527f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1740, -0.3208,  0.0265,  0.0383,  0.0932, -0.1099],\n",
       "        [-0.1526, -0.3690,  0.0630,  0.1315,  0.2067,  0.0984],\n",
       "        [-0.3225, -0.3013,  0.0294, -0.0284,  0.0787, -0.1918],\n",
       "        [-0.0835, -0.3233,  0.0186,  0.0636,  0.0451, -0.1660]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3b93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem with this is that each context vector uses isnformation from all of thje embedding vectors\n",
    "# in practice, we should only use information from the previous vectors\n",
    "# to accomplish this, we'll implement causal attention AKA masked attention\n",
    "weights = torch.randn( inputs.shape[0], inputs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0b70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559, -0.6662, -1.0584,  1.2920],\n",
       "        [-0.2979,  0.4993,  0.7955, -1.0255],\n",
       "        [ 0.2182, -0.3816,  1.8443, -0.4501],\n",
       "        [ 0.8371,  0.8238, -0.4044, -2.0110]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a9df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4886, -0.0286,  1.2308, -0.7544])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec109bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tril?\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) )\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f57ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.2979,  0.4993,  0.0000, -0.0000],\n",
       "        [ 0.2182, -0.3816,  1.8443, -0.0000],\n",
       "        [ 0.8371,  0.8238, -0.4044, -2.0110]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights*simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd480037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0559,  0.2014,  1.6809, -0.7544])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eed2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559],\n",
       "        [ 0.2014],\n",
       "        [ 1.6809],\n",
       "        [-0.7544]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked weights so that they sum to 1\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True )\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2141a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-1.4788,  2.4788,  0.0000, -0.0000],\n",
       "        [ 0.1298, -0.2270,  1.0972, -0.0000],\n",
       "        [-1.1096, -1.0919,  0.5361,  2.6655]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f72ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking mehod #2\n",
    "# torch.triu?\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal=1 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505eb036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559, -0.6662, -1.0584,  1.2920],\n",
       "        [-0.2979,  0.4993,  0.7955, -1.0255],\n",
       "        [ 0.2182, -0.3816,  1.8443, -0.4501],\n",
       "        [ 0.8371,  0.8238, -0.4044, -2.0110]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "477d81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559,    -inf,    -inf,    -inf],\n",
       "        [-0.2979,  0.4993,    -inf,    -inf],\n",
       "        [ 0.2182, -0.3816,  1.8443,    -inf],\n",
       "        [ 0.8371,  0.8238, -0.4044, -2.0110]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights.masked_fill( mask.bool(), -torch.inf)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "082c93a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3106, 0.6894, 0.0000, 0.0000],\n",
       "        [0.1508, 0.0828, 0.7665, 0.0000],\n",
       "        [0.4285, 0.4228, 0.1238, 0.0248]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6c0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36d94291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout \n",
    "# Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "# It works by randomly setting a fraction of input units to zero at each update during training time,\n",
    "# which helps to break up happenstance correlations in the training data.\n",
    "dropout = nn.Dropout( 0.5 ) # 50% dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e1eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5207e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "          0.0000e+00,  3.3747e+00,  3.9404e+00],\n",
       "        [ 1.8501e-01,  3.8569e-03, -0.0000e+00,  2.0119e-01, -0.0000e+00,\n",
       "         -7.1642e-01, -9.3683e-01,  0.0000e+00],\n",
       "        [-0.0000e+00, -2.5348e+00, -0.0000e+00, -5.2073e-02, -2.3819e-01,\n",
       "         -1.0966e+00, -1.7205e+00, -6.9962e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          2.5791e+00, -0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c6f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be able to give our LLM vbatches of input.\n",
    "# For example:\n",
    "batches = torch.stack((inputs, inputs), dim = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b0764ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack?\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "363292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to hande batches of input\n",
    "\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "        # include dropout:\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        # use the following to manage memory effeciently\n",
    "        self.register_buffer(\"mask\", torch.triu( torch.ones(context_length, context_length), diagonal = 1 ))\n",
    "        \n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # b = batch size \n",
    "\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.transpose(1, 2)\n",
    "        scores.masked_fill_( self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        weights = self.dropout( weights )\n",
    "        context = weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a64076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instiantiate a causal attention mechanism:\n",
    "causal = CausalAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "986a2135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3318, -0.2569, -0.5377,  0.4953, -0.5009,  0.4327],\n",
       "         [ 0.0201,  0.0702,  0.0479,  0.5527, -0.2927,  0.3915],\n",
       "         [ 0.0886, -0.0753, -0.0943,  0.3738, -0.0094,  0.2412],\n",
       "         [ 0.1576,  0.0944, -0.0472,  0.1699, -0.2471,  0.0433]],\n",
       "\n",
       "        [[ 0.3318, -0.2569, -0.5377,  0.4953, -0.5009,  0.4327],\n",
       "         [ 0.0201,  0.0702,  0.0479,  0.5527, -0.2927,  0.3915],\n",
       "         [ 0.0886, -0.0753, -0.0943,  0.3738, -0.0094,  0.2412],\n",
       "         [ 0.1576,  0.0944, -0.0472,  0.1699, -0.2471,  0.0433]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53553c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "W_v = nn.Linear( d_in, d_out, bias=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbf54300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.0770e-01,  4.3816e-02,  1.7545e-01, -1.1417e-01, -8.3971e-01,\n",
       "           3.6785e-01],\n",
       "         [ 1.4849e-03,  3.5108e-01,  4.7065e-01, -9.5396e-02, -2.1979e-01,\n",
       "          -9.3486e-01],\n",
       "         [-1.8468e-01, -9.3662e-02, -3.0378e-01, -4.8023e-01,  4.2630e-02,\n",
       "          -3.1774e-01],\n",
       "         [-2.5849e-01,  3.3737e-02, -5.7544e-01,  3.2325e-01,  7.3650e-01,\n",
       "           1.6680e+00]],\n",
       "\n",
       "        [[ 6.0770e-01,  4.3816e-02,  1.7545e-01, -1.1417e-01, -8.3971e-01,\n",
       "           3.6785e-01],\n",
       "         [ 1.4849e-03,  3.5108e-01,  4.7065e-01, -9.5396e-02, -2.1979e-01,\n",
       "          -9.3486e-01],\n",
       "         [-1.8468e-01, -9.3662e-02, -3.0378e-01, -4.8023e-01,  4.2630e-02,\n",
       "          -3.1774e-01],\n",
       "         [-2.5849e-01,  3.3737e-02, -5.7544e-01,  3.2325e-01,  7.3650e-01,\n",
       "           1.6680e+00]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = W_q( batches )\n",
    "queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80d1a56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2130,  0.7665,  0.2220,  0.0817,  0.2116, -0.5120],\n",
       "         [-0.0357, -0.8210,  0.7736,  0.2082,  0.4418, -0.0189],\n",
       "         [-0.3699, -0.3950,  0.0992,  0.0451, -0.0282,  0.2048],\n",
       "         [ 1.0585,  0.8586, -0.8569, -0.0166, -0.6833, -0.5157]],\n",
       "\n",
       "        [[ 0.2130,  0.7665,  0.2220,  0.0817,  0.2116, -0.5120],\n",
       "         [-0.0357, -0.8210,  0.7736,  0.2082,  0.4418, -0.0189],\n",
       "         [-0.3699, -0.3950,  0.0992,  0.0451, -0.0282,  0.2048],\n",
       "         [ 1.0585,  0.8586, -0.8569, -0.0166, -0.6833, -0.5157]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = W_k( batches )\n",
    "keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38b33094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the transpose of keys\n",
    "\n",
    "# keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5733fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a first pass at multi-head attention (not very efficient yet)\n",
    "class MultiHeadAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        return torch.cat( [ head(x) for head in self.heads ], dim=-1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c122f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5a050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbad71f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1756e+00, -5.9299e-01, -5.2680e-01, -1.8932e-01, -3.2679e-01,\n",
       "          -1.9845e-01,  2.5568e-01, -5.9830e-01, -1.3662e-01,  1.7749e-01,\n",
       "          -4.0063e-01, -2.4586e-01,  9.0462e-01,  1.5733e-01, -5.4913e-01,\n",
       "          -2.9235e-01,  7.6064e-01, -8.8952e-04],\n",
       "         [-7.6239e-01, -8.7538e-02, -2.2922e-01,  1.3593e-01, -3.4767e-01,\n",
       "          -3.7863e-01,  6.4695e-02, -1.9520e-01, -2.1597e-01,  1.0846e-01,\n",
       "          -3.8420e-01, -7.4878e-02,  2.1067e-01,  1.6160e-01, -4.2506e-01,\n",
       "          -4.0291e-01,  4.3554e-01, -5.0313e-01],\n",
       "         [-2.0241e-01,  6.4122e-02, -1.3287e-01,  1.1136e-01, -2.2071e-01,\n",
       "          -3.8027e-01, -5.7724e-03,  1.7208e-01, -3.0616e-01, -1.2038e-01,\n",
       "          -2.2491e-01, -1.9302e-02, -1.5479e-01,  1.5360e-01, -2.7218e-01,\n",
       "          -2.7348e-01,  2.5013e-01, -3.5993e-01],\n",
       "         [-3.5451e-02, -6.8312e-02, -1.2275e-01, -6.6263e-02,  8.4505e-03,\n",
       "          -2.8931e-01,  4.0998e-02,  2.5444e-01, -2.8513e-01, -1.2209e-01,\n",
       "          -5.9665e-02,  4.7883e-02,  4.7651e-02,  2.0971e-01, -2.0134e-01,\n",
       "          -1.2569e-01,  1.4683e-01, -1.6710e-01]],\n",
       "\n",
       "        [[-1.1756e+00, -5.9299e-01, -5.2680e-01, -1.8932e-01, -3.2679e-01,\n",
       "          -1.9845e-01,  2.5568e-01, -5.9830e-01, -1.3662e-01,  1.7749e-01,\n",
       "          -4.0063e-01, -2.4586e-01,  9.0462e-01,  1.5733e-01, -5.4913e-01,\n",
       "          -2.9235e-01,  7.6064e-01, -8.8952e-04],\n",
       "         [-7.6239e-01, -8.7538e-02, -2.2922e-01,  1.3593e-01, -3.4767e-01,\n",
       "          -3.7863e-01,  6.4695e-02, -1.9520e-01, -2.1597e-01,  1.0846e-01,\n",
       "          -3.8420e-01, -7.4878e-02,  2.1067e-01,  1.6160e-01, -4.2506e-01,\n",
       "          -4.0291e-01,  4.3554e-01, -5.0313e-01],\n",
       "         [-2.0241e-01,  6.4122e-02, -1.3287e-01,  1.1136e-01, -2.2071e-01,\n",
       "          -3.8027e-01, -5.7724e-03,  1.7208e-01, -3.0616e-01, -1.2038e-01,\n",
       "          -2.2491e-01, -1.9302e-02, -1.5479e-01,  1.5360e-01, -2.7218e-01,\n",
       "          -2.7348e-01,  2.5013e-01, -3.5993e-01],\n",
       "         [-3.5451e-02, -6.8312e-02, -1.2275e-01, -6.6263e-02,  8.4505e-03,\n",
       "          -2.8931e-01,  4.0998e-02,  2.5444e-01, -2.8513e-01, -1.2209e-01,\n",
       "          -5.9665e-02,  4.7883e-02,  4.7651e-02,  2.0971e-01, -2.0134e-01,\n",
       "          -1.2569e-01,  1.4683e-01, -1.6710e-01]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05cb7a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 18])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0128ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient version of multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f5a2bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6878969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01, -6.4746e-01,\n",
       "           9.2566e-01,  1.6874e+00,  1.9702e+00],\n",
       "         [ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01, -1.4050e+00,\n",
       "          -3.5821e-01, -4.6841e-01,  3.9729e-01],\n",
       "         [-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02, -1.1910e-01,\n",
       "          -5.4830e-01, -8.6023e-01, -3.4981e-01],\n",
       "         [ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00,  1.6816e+00,\n",
       "           1.2896e+00, -2.1932e-01,  1.0227e+00]],\n",
       "\n",
       "        [[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01, -6.4746e-01,\n",
       "           9.2566e-01,  1.6874e+00,  1.9702e+00],\n",
       "         [ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01, -1.4050e+00,\n",
       "          -3.5821e-01, -4.6841e-01,  3.9729e-01],\n",
       "         [-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02, -1.1910e-01,\n",
       "          -5.4830e-01, -8.6023e-01, -3.4981e-01],\n",
       "         [ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00,  1.6816e+00,\n",
       "           1.2896e+00, -2.1932e-01,  1.0227e+00]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cfcfb02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01],\n",
       "          [-6.4746e-01,  9.2566e-01,  1.6874e+00,  1.9702e+00]],\n",
       "\n",
       "         [[ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01],\n",
       "          [-1.4050e+00, -3.5821e-01, -4.6841e-01,  3.9729e-01]],\n",
       "\n",
       "         [[-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02],\n",
       "          [-1.1910e-01, -5.4830e-01, -8.6023e-01, -3.4981e-01]],\n",
       "\n",
       "         [[ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00],\n",
       "          [ 1.6816e+00,  1.2896e+00, -2.1932e-01,  1.0227e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7603e-01, -4.2865e-01, -1.4078e-01, -2.5170e-01],\n",
       "          [-6.4746e-01,  9.2566e-01,  1.6874e+00,  1.9702e+00]],\n",
       "\n",
       "         [[ 9.2507e-02,  1.9285e-03, -1.4027e+00,  1.0059e-01],\n",
       "          [-1.4050e+00, -3.5821e-01, -4.6841e-01,  3.9729e-01]],\n",
       "\n",
       "         [[-2.0567e-01, -1.2674e+00, -1.9255e-01, -2.6036e-02],\n",
       "          [-1.1910e-01, -5.4830e-01, -8.6023e-01, -3.4981e-01]],\n",
       "\n",
       "         [[ 6.5876e-01,  4.6910e-01,  2.1262e+00,  1.0740e+00],\n",
       "          [ 1.6816e+00,  1.2896e+00, -2.1932e-01,  1.0227e+00]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.view( 2, 4, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fac641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7212337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49137cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2968, -0.6419,  0.2184,  0.0102,  0.2530,  0.1569],\n",
       "         [-0.1804, -0.4610,  0.1877,  0.2508,  0.3569,  0.0738],\n",
       "         [-0.0145, -0.2382,  0.1124,  0.2263,  0.2202,  0.1077],\n",
       "         [-0.0110, -0.2989,  0.0722,  0.1058,  0.1845,  0.1893]],\n",
       "\n",
       "        [[-0.2968, -0.6419,  0.2184,  0.0102,  0.2530,  0.1569],\n",
       "         [-0.1804, -0.4610,  0.1877,  0.2508,  0.3569,  0.0738],\n",
       "         [-0.0145, -0.2382,  0.1124,  0.2263,  0.2202,  0.1077],\n",
       "         [-0.0110, -0.2989,  0.0722,  0.1058,  0.1845,  0.1893]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8924cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
