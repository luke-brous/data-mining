{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84c0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates advanced attention mechanisms in PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccce9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.nn.Embedding( 4, 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d327bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1612, -1.5821, -3.2273,  0.3622, -0.3453, -0.8032,  2.3792, -2.1280],\n",
       "        [-0.5797,  0.9732,  0.0234, -1.2803,  1.2088,  0.3766, -0.4586, -0.1949],\n",
       "        [ 0.1854, -0.5733, -0.3561,  1.3627,  0.7037, -1.2671, -0.9148,  0.3970],\n",
       "        [ 0.9707,  0.3771, -1.1129, -0.3616, -0.3282,  0.4932, -0.7722, -1.4453]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee4840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1612, -1.5821, -3.2273,  0.3622, -0.3453, -0.8032,  2.3792, -2.1280],\n",
       "        [-0.5797,  0.9732,  0.0234, -1.2803,  1.2088,  0.3766, -0.4586, -0.1949],\n",
       "        [ 0.1854, -0.5733, -0.3561,  1.3627,  0.7037, -1.2671, -0.9148,  0.3970],\n",
       "        [ 0.9707,  0.3771, -1.1129, -0.3616, -0.3282,  0.4932, -0.7722, -1.4453]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9bd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimensions\n",
    "d_in = 8\n",
    "d_out = 6\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_k = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_v = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee23407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3058, -0.3179, -2.1898,  1.3835,  1.2747, -0.2163],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an input vector and transform it into our query vector using W_q\n",
    "query = inputs[2] @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0350419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[ 1.7125,  4.7981, -6.4154, -2.7962, -1.7447,  1.8789],\n",
      "        [ 1.6495, -4.4002,  4.3578,  0.4439,  1.8209, -2.5818],\n",
      "        [-1.1307,  1.7959, -0.8145,  1.5650, -3.8818, -0.3500],\n",
      "        [ 3.2970, -2.4296,  1.2361, -1.8643, -0.7973, -1.6809]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Values: tensor([[-4.3350, -3.5431, -4.8430, -9.0924,  3.1350, -0.6456],\n",
      "        [-0.3700, -4.6019, -1.5630,  0.4270,  2.4005,  0.5838],\n",
      "        [-0.0802,  5.8840,  1.3016,  0.9462, -0.5162,  0.6040],\n",
      "        [ 2.9681, -2.7152, -2.0311, -0.6229, -1.1536, -0.7158]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\" , keys)\n",
    "print(\"Values:\" , values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2.0759,  -8.4537,   1.1126, -12.7687], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T # query is 1 by 6 and keys is 4 by 6 so we need to transpose keys\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5914, 0.0080, 0.3991, 0.0014], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim=-1 ) # the softmax function normalizes the scores\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a97eedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum() # ensure the weights sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f663f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5947,  0.2123, -2.3602, -4.9974,  1.6659, -0.1371],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ values # \n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6676af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_k = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_v = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7db99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac967da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8459,  0.8108,  0.5351,  0.9449,  1.6634, -0.0625],\n",
       "        [ 0.1150,  0.5130, -0.4308, -0.5406, -0.4469,  0.7746],\n",
       "        [-0.8703, -1.2867, -0.6136,  0.4749, -0.5217,  1.0471],\n",
       "        [ 0.0512, -0.8173, -1.4908,  1.7152,  0.3392,  1.2870],\n",
       "        [-0.0671,  1.3477, -1.9594, -1.7509,  0.6701,  1.1448],\n",
       "        [-0.1593,  1.4666,  1.2015, -0.3072, -0.3401, -1.5671],\n",
       "        [ 0.3325,  1.1337, -0.4178,  0.6868, -0.5777, -1.9540],\n",
       "        [ 2.8169,  1.9666,  1.5134, -2.5585, -1.1444,  0.6638]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb72a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.6973,  -3.5885,  -1.5028,  -1.2160,   4.6796,  -1.6612],\n",
       "        [ -0.2407,  -1.0286,   2.1604,  -1.0801,  -1.7408,  -0.6134],\n",
       "        [  1.3794,   3.5763,   5.0256,  -8.9521, -11.1206,  -6.5928],\n",
       "        [ -0.9972,  -3.9482,  -3.1072,   2.8289,  -2.4738,  -0.1428]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f76a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second version of the class\n",
    "# it uses nn.Linear to do things more effectively\n",
    "\n",
    "class SimpleAttentionv2( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttentionv2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e527f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0775, -0.3132, -0.1022,  0.1780, -0.2306, -0.2533],\n",
       "        [ 0.1155, -0.3383, -0.0810,  0.2060, -0.2622, -0.3131],\n",
       "        [ 0.1036, -0.2941, -0.1911,  0.1618, -0.2324, -0.1353],\n",
       "        [ 0.0893, -0.2930, -0.1805,  0.1577, -0.2208, -0.1400]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3b93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem with this is that each context vector uses isnformation from all of thje embedding vectors\n",
    "# in practice, we should only use information from the previous vectors\n",
    "# to accomplish this, we'll implement causal attention AKA masked attention\n",
    "weights = torch.randn( inputs.shape[0], inputs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0b70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8963, -0.8987, -0.6772,  2.1312],\n",
       "        [-0.0254,  0.3243,  2.0620,  0.2009],\n",
       "        [-1.6154,  0.2787, -0.9579, -1.4212],\n",
       "        [ 1.6788,  0.1325,  0.0710, -1.3858]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a9df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4515,  2.5617, -3.7158,  0.4965])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec109bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tril?\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) )\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f57ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8963, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0254,  0.3243,  0.0000,  0.0000],\n",
       "        [-1.6154,  0.2787, -0.9579, -0.0000],\n",
       "        [ 1.6788,  0.1325,  0.0710, -1.3858]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights*simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd480037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8963,  0.2988, -2.2946,  0.4965])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eed2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8963],\n",
       "        [ 0.2988],\n",
       "        [-2.2946],\n",
       "        [ 0.4965]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked weights so that they sum to 1\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True )\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2141a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0851,  1.0851,  0.0000,  0.0000],\n",
       "        [ 0.7040, -0.1215,  0.4175,  0.0000],\n",
       "        [ 3.3815,  0.2668,  0.1431, -2.7914]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f72ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking mehod #2\n",
    "# torch.triu?\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal=1 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505eb036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8963, -0.8987, -0.6772,  2.1312],\n",
       "        [-0.0254,  0.3243,  2.0620,  0.2009],\n",
       "        [-1.6154,  0.2787, -0.9579, -1.4212],\n",
       "        [ 1.6788,  0.1325,  0.0710, -1.3858]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "477d81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8963,    -inf,    -inf,    -inf],\n",
       "        [-0.0254,  0.3243,    -inf,    -inf],\n",
       "        [-1.6154,  0.2787, -0.9579,    -inf],\n",
       "        [ 1.6788,  0.1325,  0.0710, -1.3858]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights.masked_fill( mask.bool(), -torch.inf)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "082c93a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4135, 0.5865, 0.0000, 0.0000],\n",
       "        [0.1044, 0.6941, 0.2015, 0.0000],\n",
       "        [0.6849, 0.1459, 0.1372, 0.0320]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6c0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36d94291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout \n",
    "# Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "# It works by randomly setting a fraction of input units to zero at each update during training time,\n",
    "# which helps to break up happenstance correlations in the training data.\n",
    "dropout = nn.Dropout( 0.5 ) # 50% dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e1eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3225, -0.0000, -6.4547,  0.0000, -0.0000, -0.0000,  0.0000, -4.2560],\n",
       "        [-0.0000,  1.9463,  0.0000, -2.5606,  0.0000,  0.7532, -0.9171, -0.3897],\n",
       "        [ 0.0000, -1.1466, -0.7122,  2.7253,  0.0000, -2.5342, -0.0000,  0.0000],\n",
       "        [ 1.9414,  0.0000, -2.2259, -0.7233, -0.0000,  0.0000, -1.5445, -2.8906]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c6f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be able to give our LLM vbatches of input.\n",
    "# For example:\n",
    "batches = torch.stack((inputs, inputs), dim = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b0764ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack?\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "363292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to hande batches of input\n",
    "\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "        # include dropout:\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        # use the following to manage memory effeciently\n",
    "        self.register_buffer(\"mask\", torch.triu( torch.ones(context_length, context_length), diagonal = 1 ))\n",
    "        \n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # b = batch size \n",
    "\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.transpose(1, 2)\n",
    "        scores.masked_fill_( self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        weights = self.dropout( weights )\n",
    "        context = weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceddd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a64076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instiantiate a causal attention mechanism:\n",
    "causal = CausalAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "986a2135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3831, -1.2413, -0.1268, -0.4254,  1.2755, -1.2354],\n",
       "         [ 0.4137, -0.3700, -0.3301, -0.3596,  0.8100, -0.5308],\n",
       "         [ 0.1762,  0.0235, -0.1443, -0.4402,  0.7742, -0.3843],\n",
       "         [ 0.2175, -0.3614,  0.1000, -0.4856,  0.6779, -0.6570]],\n",
       "\n",
       "        [[ 1.3831, -1.2413, -0.1268, -0.4254,  1.2755, -1.2354],\n",
       "         [ 0.4137, -0.3700, -0.3301, -0.3596,  0.8100, -0.5308],\n",
       "         [ 0.1762,  0.0235, -0.1443, -0.4402,  0.7742, -0.3843],\n",
       "         [ 0.2175, -0.3614,  0.1000, -0.4856,  0.6779, -0.6570]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53553c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "W_v = nn.Linear( d_in, d_out, bias=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbf54300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4746,  0.4328, -0.0162,  1.2288,  0.3441, -0.3742],\n",
       "         [-0.2100, -0.7356,  0.3681,  0.7959,  0.2213, -0.0428],\n",
       "         [-0.2277,  0.1888, -0.9839, -0.3556,  0.9933, -0.4919],\n",
       "         [ 0.5435,  0.1198,  0.3716,  0.1646,  0.3668, -0.1627]],\n",
       "\n",
       "        [[-0.4746,  0.4328, -0.0162,  1.2288,  0.3441, -0.3742],\n",
       "         [-0.2100, -0.7356,  0.3681,  0.7959,  0.2213, -0.0428],\n",
       "         [-0.2277,  0.1888, -0.9839, -0.3556,  0.9933, -0.4919],\n",
       "         [ 0.5435,  0.1198,  0.3716,  0.1646,  0.3668, -0.1627]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = W_q( batches )\n",
    "queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80d1a56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6473, -0.2471, -1.1271, -0.1390,  0.6564,  1.7676],\n",
       "         [ 0.4730, -0.0504, -0.2020,  0.2622, -0.5492,  0.5974],\n",
       "         [-0.3610,  0.2776, -0.6320,  0.4486,  0.1768, -0.2313],\n",
       "         [-0.0405, -0.0749,  0.4225, -0.1620, -0.2219, -0.1709]],\n",
       "\n",
       "        [[-0.6473, -0.2471, -1.1271, -0.1390,  0.6564,  1.7676],\n",
       "         [ 0.4730, -0.0504, -0.2020,  0.2622, -0.5492,  0.5974],\n",
       "         [-0.3610,  0.2776, -0.6320,  0.4486,  0.1768, -0.2313],\n",
       "         [-0.0405, -0.0749,  0.4225, -0.1620, -0.2219, -0.1709]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = W_k( batches )\n",
    "keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b33094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the transpose of keys\n",
    "\n",
    "# keys.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
