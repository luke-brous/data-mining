{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84c0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates advanced attention mechanisms in PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccce9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer with 4 possible inputs and an embedding dimension of 8\n",
    "inputs = torch.nn.Embedding( 4, 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d327bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7134, -0.3138,  0.5046, -0.8650,  0.4425, -1.1841, -0.7362,  1.1409],\n",
       "        [ 1.0442, -0.3844,  0.0344,  0.7320, -1.7047,  0.8123,  1.1130, -1.3390],\n",
       "        [ 0.9229,  0.6272,  0.8922, -1.2379, -1.0845, -1.8604, -0.1238,  0.6430],\n",
       "        [ 0.1835, -1.5439, -0.5757, -0.6085,  0.9697,  1.0065,  0.5641,  0.1937]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the weights of the embedding layer\n",
    "inputs = inputs.weight\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee4840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7134, -0.3138,  0.5046, -0.8650,  0.4425, -1.1841, -0.7362,  1.1409],\n",
       "        [ 1.0442, -0.3844,  0.0344,  0.7320, -1.7047,  0.8123,  1.1130, -1.3390],\n",
       "        [ 0.9229,  0.6272,  0.8922, -1.2379, -1.0845, -1.8604, -0.1238,  0.6430],\n",
       "        [ 0.1835, -1.5439, -0.5757, -0.6085,  0.9697,  1.0065,  0.5641,  0.1937]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the weights to a tensor\n",
    "inputs = inputs.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9bd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimensions\n",
    "d_in = 8\n",
    "d_out = 6\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_k = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_v = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee23407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8183,  2.5620, -2.9488,  4.0440, -0.8484, -0.2352],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an input vector and transform it into our query vector using W_q\n",
    "query = inputs[2] @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0350419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[-3.7374,  1.8343, -1.3490, -0.3107,  1.2861,  0.6717],\n",
      "        [ 7.2285, -1.7452,  1.8141, -2.9936, -3.2840, -0.8258],\n",
      "        [ 2.1841,  4.8209, -2.9804, -3.8569, -1.9949,  4.4026],\n",
      "        [-2.2238, -0.4988,  0.9372, -1.4520,  0.2754, -5.0220]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Values: tensor([[-3.6430, -1.5747, -3.4956, -0.2345, -4.3542, -2.8565],\n",
      "        [ 3.3199,  1.3926,  7.3296, -0.6822,  3.6932,  3.9693],\n",
      "        [-2.7182, -1.0483,  0.5692, -1.3523, -5.8348, -1.6021],\n",
      "        [-0.8293, -0.2994, -1.2222,  1.6421,  2.3553, -1.9783]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\" , keys)\n",
    "print(\"Values:\" , values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 12.9675, -32.0893,   2.2283,  -4.9221], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T # query is 1 by 6 and keys is 4 by 6 so we need to transpose keys\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.8703e-01, 1.0133e-08, 1.2310e-02, 6.6450e-04],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim=-1 ) # the softmax function normalizes the scores\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a97eedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum() # ensure the weights sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f663f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6298, -1.5674, -3.4440, -0.2470, -4.3679, -2.8404],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the context vector as a weighted sum of the values\n",
    "context_vector = attention_weights @ values \n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6676af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple attention module\n",
    "class SimpleAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_k = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_v = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7db99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac967da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-5.1242e-01,  2.0099e+00,  3.1250e-01, -1.9914e-01, -2.2349e+00,\n",
       "         -4.9228e-01],\n",
       "        [-3.2044e+00, -8.1958e-01,  9.9606e-01,  4.5301e-01,  4.3805e-02,\n",
       "         -4.2438e-01],\n",
       "        [ 3.8090e-01,  1.0083e+00, -1.1598e+00, -2.2863e-01, -5.0853e-01,\n",
       "         -1.1355e+00],\n",
       "        [ 8.0831e-01,  1.2780e+00,  1.1620e+00,  1.1173e-01,  4.7857e-01,\n",
       "          4.2691e-01],\n",
       "        [ 1.6157e-01,  1.1227e+00, -4.5885e-01, -3.5608e-01, -1.5787e+00,\n",
       "          4.1086e-01],\n",
       "        [-6.2358e-01, -3.4047e-01, -1.0906e+00, -2.4596e-03,  1.6934e-01,\n",
       "          4.8300e-01],\n",
       "        [-1.3639e+00,  1.8232e+00,  1.4452e+00,  1.0897e-01, -1.4025e+00,\n",
       "         -1.1445e+00],\n",
       "        [ 2.2419e+00,  6.5118e-02, -1.4556e+00, -7.6229e-01,  5.4553e-01,\n",
       "         -5.8214e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "simple.W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb72a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029,  2.7619, -1.6765, -1.7295, -1.0699,  1.2854],\n",
       "        [-1.6005, -0.0114,  1.0342,  1.8843, -3.2254,  3.8953],\n",
       "        [ 0.6028,  2.7617, -1.6766, -1.7293, -1.0703,  1.2854],\n",
       "        [-1.5524, -0.4926,  2.6534,  0.8774,  2.8478, -0.5394]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the context vectors by passing the input embeddings to the attention module\n",
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f76a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second version of the class\n",
    "# it uses nn.Linear to do things more effectively\n",
    "\n",
    "class SimpleAttentionv2( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttentionv2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e527f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0360,  0.3353, -0.4085, -0.0902, -0.0380,  0.1301],\n",
       "        [-0.0656,  0.3166, -0.4281, -0.0242, -0.0819,  0.1063],\n",
       "        [-0.2505,  0.4939, -0.3197,  0.0262, -0.0536,  0.1213],\n",
       "        [-0.0275,  0.3167, -0.4300, -0.0818, -0.0508,  0.1211]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa3b93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem with this is that each context vector uses isnformation from all of thje embedding vectors\n",
    "# in practice, we should only use information from the previous vectors\n",
    "# to accomplish this, we'll implement causal attention AKA masked attention\n",
    "weights = torch.randn( inputs.shape[0], inputs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0b70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6461, -0.6025,  0.1016,  0.6006],\n",
       "        [ 0.2894,  0.8791,  0.1680,  1.7750],\n",
       "        [-2.7479, -0.3686, -0.1043,  1.2650],\n",
       "        [ 0.3311, -1.2953, -1.6291,  0.2144]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a9df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5464,  3.1116, -1.9557, -2.3789])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec109bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tril?\n",
    "# Create a lower triangular mask to apply causal attention\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) )\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f57ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6461, -0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2894,  0.8791,  0.0000,  0.0000],\n",
       "        [-2.7479, -0.3686, -0.1043,  0.0000],\n",
       "        [ 0.3311, -1.2953, -1.6291,  0.2144]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mask to the weights\n",
    "masked_weights = weights*simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd480037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6461,  1.1686, -3.2207, -2.3789])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eed2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6461],\n",
       "        [ 1.1686],\n",
       "        [-3.2207],\n",
       "        [-2.3789]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked weights so that they sum to 1\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True )\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2141a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.2477,  0.7523,  0.0000,  0.0000],\n",
       "        [ 0.8532,  0.1144,  0.0324, -0.0000],\n",
       "        [-0.1392,  0.5445,  0.6848, -0.0901]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the masked weights by the sum of each row\n",
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f72ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking method #2\n",
    "# torch.triu?\n",
    "# use the upper triangular part of the matrix to create a mask\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal=1 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505eb036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6461, -0.6025,  0.1016,  0.6006],\n",
       "        [ 0.2894,  0.8791,  0.1680,  1.7750],\n",
       "        [-2.7479, -0.3686, -0.1043,  1.2650],\n",
       "        [ 0.3311, -1.2953, -1.6291,  0.2144]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "477d81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6461,    -inf,    -inf,    -inf],\n",
       "        [ 0.2894,  0.8791,    -inf,    -inf],\n",
       "        [-2.7479, -0.3686, -0.1043,    -inf],\n",
       "        [ 0.3311, -1.2953, -1.6291,  0.2144]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the mask to the weights by setting the masked positions to -infinity\n",
    "weights = weights.masked_fill( mask.bool(), -torch.inf)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "082c93a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3567, 0.6433, 0.0000, 0.0000],\n",
       "        [0.0387, 0.4175, 0.5438, 0.0000],\n",
       "        [0.4490, 0.0883, 0.0632, 0.3995]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the softmax function to the masked weights\n",
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6c0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36d94291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout \n",
    "# Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "# It works by randomly setting a fraction of input units to zero at each update during training time,\n",
    "# which helps to break up happenstance correlations in the training data.\n",
    "dropout = nn.Dropout( 0.5 ) # 50% dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e1eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4267, -0.6277,  1.0092, -1.7300,  0.8851, -2.3683, -1.4723,  2.2819],\n",
       "        [ 2.0884, -0.0000,  0.0687,  0.0000, -0.0000,  1.6247,  0.0000, -2.6780],\n",
       "        [ 0.0000,  1.2545,  0.0000, -0.0000, -2.1691, -3.7208, -0.2475,  0.0000],\n",
       "        [ 0.3669, -3.0878, -0.0000, -1.2170,  1.9395,  2.0130,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6c6f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be able to give our LLM vbatches of input.\n",
    "# For example:\n",
    "batches = torch.stack((inputs, inputs), dim = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b0764ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack?\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "363292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to hande batches of input\n",
    "\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "        # include dropout:\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        # use the following to manage memory effeciently\n",
    "        self.register_buffer(\"mask\", torch.triu( torch.ones(context_length, context_length), diagonal = 1 ))\n",
    "        \n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # b = batch size \n",
    "\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.transpose(1, 2)\n",
    "        scores.masked_fill_( self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        weights = self.dropout( weights )\n",
    "        context = weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a64076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instiantiate a causal attention mechanism:\n",
    "causal = CausalAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "986a2135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5626,  0.4381,  0.5288,  0.2468,  0.3463,  0.7203],\n",
       "         [ 0.2169,  0.0861,  0.2949, -0.1872, -0.4878, -0.2642],\n",
       "         [-0.1225,  0.3905,  0.5408, -0.0017, -0.0463,  0.4485],\n",
       "         [-0.3033,  0.2605,  0.3082, -0.3160, -0.1470,  0.3007]],\n",
       "\n",
       "        [[-0.5626,  0.4381,  0.5288,  0.2468,  0.3463,  0.7203],\n",
       "         [ 0.2169,  0.0861,  0.2949, -0.1872, -0.4878, -0.2642],\n",
       "         [-0.1225,  0.3905,  0.5408, -0.0017, -0.0463,  0.4485],\n",
       "         [-0.3033,  0.2605,  0.3082, -0.3160, -0.1470,  0.3007]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the batches of input to the causal attention mechanism\n",
    "causal( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53553c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear layers for query, key, and value projections\n",
    "W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "W_v = nn.Linear( d_in, d_out, bias=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbf54300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2314, -0.1071,  0.1299,  0.5449,  0.7863,  0.5378],\n",
       "         [ 0.1127,  0.4679, -0.7802, -0.8144, -1.0449, -0.4840],\n",
       "         [-0.4056, -0.3382, -0.6517, -0.4530, -0.3455,  0.8290],\n",
       "         [ 0.5569,  0.6049,  0.6544,  0.2646,  0.1090, -0.3899]],\n",
       "\n",
       "        [[-0.2314, -0.1071,  0.1299,  0.5449,  0.7863,  0.5378],\n",
       "         [ 0.1127,  0.4679, -0.7802, -0.8144, -1.0449, -0.4840],\n",
       "         [-0.4056, -0.3382, -0.6517, -0.4530, -0.3455,  0.8290],\n",
       "         [ 0.5569,  0.6049,  0.6544,  0.2646,  0.1090, -0.3899]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = W_q( batches )\n",
    "queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80d1a56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1995,  0.4013,  0.4363, -0.4461,  0.0416,  0.3854],\n",
       "         [-0.1706, -0.4734, -0.2557,  0.4549, -0.1253, -0.3865],\n",
       "         [-1.3976,  0.1492,  0.1313, -0.9270, -0.6716, -0.5799],\n",
       "         [ 0.9769, -0.6248,  0.2957,  0.6422,  0.5827,  0.3082]],\n",
       "\n",
       "        [[-0.1995,  0.4013,  0.4363, -0.4461,  0.0416,  0.3854],\n",
       "         [-0.1706, -0.4734, -0.2557,  0.4549, -0.1253, -0.3865],\n",
       "         [-1.3976,  0.1492,  0.1313, -0.9270, -0.6716, -0.5799],\n",
       "         [ 0.9769, -0.6248,  0.2957,  0.6422,  0.5827,  0.3082]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = W_k( batches )\n",
    "keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38b33094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the transpose of keys\n",
    "\n",
    "# keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5733fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a first pass at multi-head attention (not very efficient yet)\n",
    "class MultiHeadAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        return torch.cat( [ head(x) for head in self.heads ], dim=-1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c122f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a multi-head attention mechanism:\n",
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5a050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the batches of input to the multi-head attention mechanism\n",
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbad71f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4055, -0.4930,  0.4081, -0.3899,  1.0181,  0.0141, -0.3746,\n",
       "          -0.1924, -0.1201, -0.5103,  0.2540, -0.1809,  0.6103,  0.3714,\n",
       "          -0.3196,  0.0750, -0.3010,  0.1979],\n",
       "         [-0.2147, -0.3419,  0.3811, -0.2669,  0.5802,  0.1264, -0.0176,\n",
       "          -0.2792,  0.0234, -0.1230,  0.1672, -0.0025, -0.0643, -0.2203,\n",
       "          -0.0196,  0.2090, -0.0799,  0.2043],\n",
       "         [-0.1336, -0.1653,  0.5755, -0.0134, -0.1535,  0.5064, -0.2484,\n",
       "          -0.4976, -0.0326, -0.0932,  0.2035, -0.2161, -0.0228, -0.1837,\n",
       "          -0.1181,  0.3191, -0.1306,  0.3001],\n",
       "         [-0.0582, -0.1563,  0.4592, -0.2106,  0.3428,  0.2693, -0.0687,\n",
       "          -0.3354,  0.2976, -0.1127,  0.1458,  0.0727, -0.0300, -0.2310,\n",
       "           0.1396,  0.2900, -0.1200,  0.2233]],\n",
       "\n",
       "        [[-0.4055, -0.4930,  0.4081, -0.3899,  1.0181,  0.0141, -0.3746,\n",
       "          -0.1924, -0.1201, -0.5103,  0.2540, -0.1809,  0.6103,  0.3714,\n",
       "          -0.3196,  0.0750, -0.3010,  0.1979],\n",
       "         [-0.2147, -0.3419,  0.3811, -0.2669,  0.5802,  0.1264, -0.0176,\n",
       "          -0.2792,  0.0234, -0.1230,  0.1672, -0.0025, -0.0643, -0.2203,\n",
       "          -0.0196,  0.2090, -0.0799,  0.2043],\n",
       "         [-0.1336, -0.1653,  0.5755, -0.0134, -0.1535,  0.5064, -0.2484,\n",
       "          -0.4976, -0.0326, -0.0932,  0.2035, -0.2161, -0.0228, -0.1837,\n",
       "          -0.1181,  0.3191, -0.1306,  0.3001],\n",
       "         [-0.0582, -0.1563,  0.4592, -0.2106,  0.3428,  0.2693, -0.0687,\n",
       "          -0.3354,  0.2976, -0.1127,  0.1458,  0.0727, -0.0300, -0.2310,\n",
       "           0.1396,  0.2900, -0.1200,  0.2233]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05cb7a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 18])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0128ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient version of multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \n",
    "        super().__init__() # Call the parent class's constructor\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a causal mask to prevent attention to future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f5a2bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6878969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7134, -0.3138,  0.5046, -0.8650,  0.4425, -1.1841, -0.7362,\n",
       "           1.1409],\n",
       "         [ 1.0442, -0.3844,  0.0344,  0.7320, -1.7047,  0.8123,  1.1130,\n",
       "          -1.3390],\n",
       "         [ 0.9229,  0.6272,  0.8922, -1.2379, -1.0845, -1.8604, -0.1238,\n",
       "           0.6430],\n",
       "         [ 0.1835, -1.5439, -0.5757, -0.6085,  0.9697,  1.0065,  0.5641,\n",
       "           0.1937]],\n",
       "\n",
       "        [[-0.7134, -0.3138,  0.5046, -0.8650,  0.4425, -1.1841, -0.7362,\n",
       "           1.1409],\n",
       "         [ 1.0442, -0.3844,  0.0344,  0.7320, -1.7047,  0.8123,  1.1130,\n",
       "          -1.3390],\n",
       "         [ 0.9229,  0.6272,  0.8922, -1.2379, -1.0845, -1.8604, -0.1238,\n",
       "           0.6430],\n",
       "         [ 0.1835, -1.5439, -0.5757, -0.6085,  0.9697,  1.0065,  0.5641,\n",
       "           0.1937]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfcfb02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7134, -0.3138,  0.5046, -0.8650],\n",
       "          [ 0.4425, -1.1841, -0.7362,  1.1409]],\n",
       "\n",
       "         [[ 1.0442, -0.3844,  0.0344,  0.7320],\n",
       "          [-1.7047,  0.8123,  1.1130, -1.3390]],\n",
       "\n",
       "         [[ 0.9229,  0.6272,  0.8922, -1.2379],\n",
       "          [-1.0845, -1.8604, -0.1238,  0.6430]],\n",
       "\n",
       "         [[ 0.1835, -1.5439, -0.5757, -0.6085],\n",
       "          [ 0.9697,  1.0065,  0.5641,  0.1937]]],\n",
       "\n",
       "\n",
       "        [[[-0.7134, -0.3138,  0.5046, -0.8650],\n",
       "          [ 0.4425, -1.1841, -0.7362,  1.1409]],\n",
       "\n",
       "         [[ 1.0442, -0.3844,  0.0344,  0.7320],\n",
       "          [-1.7047,  0.8123,  1.1130, -1.3390]],\n",
       "\n",
       "         [[ 0.9229,  0.6272,  0.8922, -1.2379],\n",
       "          [-1.0845, -1.8604, -0.1238,  0.6430]],\n",
       "\n",
       "         [[ 0.1835, -1.5439, -0.5757, -0.6085],\n",
       "          [ 0.9697,  1.0065,  0.5641,  0.1937]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.view( 2, 4, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fac641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length = 4, dropout = 0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7212337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49137cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4650,  0.6056, -0.5900,  0.4056, -0.4189,  0.0184],\n",
       "         [ 0.0365, -0.0218, -0.3391, -0.1822,  0.1810, -0.0429],\n",
       "         [ 0.0910,  0.3097, -0.3078,  0.0245, -0.1352, -0.1352],\n",
       "         [ 0.0068,  0.0135, -0.2639, -0.2703, -0.0235, -0.1038]],\n",
       "\n",
       "        [[ 0.4650,  0.6056, -0.5900,  0.4056, -0.4189,  0.0184],\n",
       "         [ 0.0365, -0.0218, -0.3391, -0.1822,  0.1810, -0.0429],\n",
       "         [ 0.0910,  0.3097, -0.3078,  0.0245, -0.1352, -0.1352],\n",
       "         [ 0.0068,  0.0135, -0.2639, -0.2703, -0.0235, -0.1038]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8924cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
