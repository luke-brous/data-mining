{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84c0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates advanced attention mechanisms in PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccce9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.nn.Embedding( 4, 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d327bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4968,  0.6926, -0.4238,  1.7006,  0.8141, -1.3960, -0.2556,  0.7842],\n",
       "        [ 1.1958, -0.4735,  2.7358,  0.9718,  0.2161,  0.9777,  0.2735,  0.3263],\n",
       "        [-0.2885,  0.7681, -0.9142,  1.6997, -0.3183,  0.9168,  1.3026,  1.1396],\n",
       "        [-0.0515, -0.7303, -1.1646,  0.7935,  1.5031, -1.0235, -1.5992, -1.2287]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee4840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4968,  0.6926, -0.4238,  1.7006,  0.8141, -1.3960, -0.2556,  0.7842],\n",
       "        [ 1.1958, -0.4735,  2.7358,  0.9718,  0.2161,  0.9777,  0.2735,  0.3263],\n",
       "        [-0.2885,  0.7681, -0.9142,  1.6997, -0.3183,  0.9168,  1.3026,  1.1396],\n",
       "        [-0.0515, -0.7303, -1.1646,  0.7935,  1.5031, -1.0235, -1.5992, -1.2287]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9bd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimensions\n",
    "d_in = 8\n",
    "d_out = 6\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_k = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "W_v = torch.nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee23407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2193, -0.9882,  4.0323,  0.0244, -0.5920,  1.6198],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an input vector and transform it into our query vector using W_q\n",
    "query = inputs[2] @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0350419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[ 0.2674, -6.3490,  1.4153, -3.2161,  3.5052, -0.8679],\n",
      "        [ 0.1761,  3.8460, -5.1995,  3.9411,  6.8248,  2.9961],\n",
      "        [-3.9272, -1.7813,  0.9085, -0.8080, -0.2954, -2.5784],\n",
      "        [-2.3890, -4.8283,  2.2408, -0.4120,  1.2446,  2.2104]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Values: tensor([[-3.2175,  1.3969,  1.8347, -6.6828, -2.0814, -3.4128],\n",
      "        [ 1.2381,  3.1518, -1.1033, -1.8141,  2.7603,  3.0491],\n",
      "        [ 2.4455,  1.9790, -1.1088, -3.3406, -2.5717, -1.4244],\n",
      "        [-7.0287, -0.4542, -0.0826, -2.3696,  0.7345, -3.2982]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\" , keys)\n",
    "print(\"Values:\" , values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  8.4802, -23.8185,   0.5411,  16.1166], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T # query is 1 by 6 and keys is 4 by 6 so we need to transpose keys\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2320e-02, 7.9428e-08, 1.6555e-03, 9.5602e-01],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim=-1 ) # the softmax function normalizes the scores\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a97eedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum() # ensure the weights sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f663f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.8517e+00, -3.7180e-01, -3.1472e-03, -2.5538e+00,  6.0983e-01,\n",
       "        -3.2999e+00], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ values # \n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6676af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_k = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "        self.W_v = nn.Parameter( torch.randn( (d_in, d_out), requires_grad=False ) )\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7db99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac967da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0579,  1.1436, -0.0773,  0.1684, -0.0646, -0.0638],\n",
       "        [ 0.5471,  0.4694, -0.2080,  0.6245,  0.1008, -2.5456],\n",
       "        [ 0.3322,  0.6964,  0.1910, -1.0184, -0.0323, -0.0416],\n",
       "        [-0.8376,  1.0151, -0.6779, -0.3007, -1.6688,  0.9362],\n",
       "        [ 0.5515, -0.4291,  0.8234, -1.7246, -0.5022, -0.2536],\n",
       "        [ 0.3747, -0.5254,  0.1214,  1.1081,  0.0311, -0.0906],\n",
       "        [ 0.2871, -0.4487, -0.3190, -2.1846,  1.1565, -0.4737],\n",
       "        [-0.7714, -0.1812, -0.9762,  0.3434,  0.7973,  0.8360]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb72a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8264,  7.0638,  1.7128, -1.2320,  0.4152, -2.8836],\n",
       "        [ 5.5971,  2.6917,  6.6572,  0.4358, -3.1679,  0.3786],\n",
       "        [ 0.6389,  6.7151,  1.7218, -1.3439,  0.3465, -2.8945],\n",
       "        [-2.7950,  2.3704, -0.5065, -3.8887,  1.1537, -1.4473]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f76a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second version of the class\n",
    "# it uses nn.Linear to do things more effectively\n",
    "\n",
    "class SimpleAttentionv2( nn.Module ):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db3cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# instance of the class\n",
    "simple = SimpleAttentionv2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7e527f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2607,  0.2483,  0.3032,  0.3527, -0.4821, -0.0535],\n",
       "        [-0.0988,  0.1871,  0.1946,  0.4106, -0.3638,  0.0158],\n",
       "        [-0.3057,  0.2415,  0.3584,  0.2557, -0.5409, -0.1973],\n",
       "        [-0.1710,  0.1838,  0.2501,  0.3458, -0.5099, -0.1348]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa3b93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem with this is that each context vector uses isnformation from all of thje embedding vectors\n",
    "# in practice, we should only use information from the previous vectors\n",
    "# to accomplish this, we'll implement causal attention AKA masked attention\n",
    "weights = torch.randn( inputs.shape[0], inputs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a0b70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3369,  0.7203,  0.6539, -0.3114],\n",
       "        [-0.4438, -1.1245, -0.7097,  1.1474],\n",
       "        [ 0.3388, -0.3955, -0.5225,  0.1182],\n",
       "        [-0.0090,  0.5231, -0.8770,  0.2381]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a9df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2740, -1.1307, -0.4610, -0.1249])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ec109bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tril?\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) )\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f57ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3369,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.4438, -1.1245, -0.0000,  0.0000],\n",
       "        [ 0.3388, -0.3955, -0.5225,  0.0000],\n",
       "        [-0.0090,  0.5231, -0.8770,  0.2381]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights*simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd480037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3369, -1.5683, -0.5792, -0.1249])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7eed2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3369],\n",
       "        [-1.5683],\n",
       "        [-0.5792],\n",
       "        [-0.1249]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked weights so that they sum to 1\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True )\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2141a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.2830,  0.7170,  0.0000, -0.0000],\n",
       "        [-0.5849,  0.6828,  0.9020, -0.0000],\n",
       "        [ 0.0722, -4.1888,  7.0229, -1.9063]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76f72ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking mehod #2\n",
    "# torch.triu?\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal=1 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0b216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "505eb036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3369,  0.7203,  0.6539, -0.3114],\n",
       "        [-0.4438, -1.1245, -0.7097,  1.1474],\n",
       "        [ 0.3388, -0.3955, -0.5225,  0.1182],\n",
       "        [-0.0090,  0.5231, -0.8770,  0.2381]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "477d81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3369,    -inf,    -inf,    -inf],\n",
       "        [-0.4438, -1.1245,    -inf,    -inf],\n",
       "        [ 0.3388, -0.3955, -0.5225,    -inf],\n",
       "        [-0.0090,  0.5231, -0.8770,  0.2381]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights.masked_fill( mask.bool(), -torch.inf)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "082c93a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6639, 0.3361, 0.0000, 0.0000],\n",
       "        [0.5256, 0.2522, 0.2222, 0.0000],\n",
       "        [0.2271, 0.3867, 0.0954, 0.2908]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f6c0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36d94291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout \n",
    "# Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "# It works by randomly setting a fraction of input units to zero at each update during training time,\n",
    "# which helps to break up happenstance correlations in the training data.\n",
    "dropout = nn.Dropout( 0.5 ) # 50% dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e1eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.3852, -0.8477,  3.4012,  0.0000, -2.7921, -0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  1.9436,  0.4322,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  3.3994, -0.0000,  1.8336,  0.0000,  2.2793],\n",
       "        [-0.1031, -1.4606, -2.3293,  0.0000,  0.0000, -0.0000, -3.1984, -2.4574]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6c6f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be able to give our LLM vbatches of input.\n",
    "# For example:\n",
    "batches = torch.stack((inputs, inputs), dim = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b0764ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack?\n",
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363292bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to hande batches of input\n",
    "\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #create weight matrices\n",
    "        self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "\n",
    "    # x = embedding vectors (inputs)\n",
    "    def forward( self, x ):\n",
    "        queries = self.W_q( x )\n",
    "        keys = self.W_k( x )\n",
    "        values = self.W_v( x )\n",
    "        scores = queries @ keys.T\n",
    "        weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "        context = weights @ values\n",
    "        return context; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80d1a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m super(self, /, *args, **kwargs)\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "super() -> same as super(__class__, <first argument>)\n",
      "super(type) -> unbound super object\n",
      "super(type, obj) -> bound super object; requires isinstance(obj, type)\n",
      "super(type, type2) -> bound super object; requires issubclass(type2, type)\n",
      "Typical use to call a cooperative superclass method:\n",
      "class C(B):\n",
      "    def meth(self, arg):\n",
      "        super().meth(arg)\n",
      "This works for class methods too:\n",
      "class C(B):\n",
      "    @classmethod\n",
      "    def cmeth(cls, arg):\n",
      "        super().cmeth(arg)\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "super?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
