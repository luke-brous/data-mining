{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f592fbb0",
   "metadata": {},
   "source": [
    "Luke Broussard \n",
    "CS290 \n",
    "Prof. Rudd \n",
    "\n",
    "\n",
    "# Tokenization \n",
    "\n",
    "\n",
    "_Tokenization_ is the process of converting a body of text into individual _tokens_, e.g., words and punctuation characters. This is the first step for most Natural Language Processing (NLP) tasks, including preparing data for training an LLM. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f17e49",
   "metadata": {},
   "source": [
    "## Some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc42f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test! Or is this not a test? Test it to be sure. :)\n",
      "This sample text has 61 characters.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test! Or is this not a test? Test it to be sure. :)\"\n",
    "print(text)\n",
    "print(f\"This sample text has {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ce35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test!', 'Or', 'is', 'this', 'not', 'a', 'test?', 'Test', 'it', 'to', 'be', 'sure.', ':)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m str.split(self, /, sep=\u001b[38;5;28;01mNone\u001b[39;00m, maxsplit=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "  sep\n",
      "    The separator used to split the string.\n",
      "\n",
      "    When set to None (the default value), will split on any whitespace\n",
      "    character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "    empty strings from the result.\n",
      "  maxsplit\n",
      "    Maximum number of splits (starting from the left).\n",
      "    -1 (the default value) means no limit.\n",
      "\n",
      "Note, str.split() is mainly useful for data that has been intentionally\n",
      "delimited.  With natural text that includes punctuation, consider using\n",
      "the regular expression module.\n",
      "\u001b[31mType:\u001b[39m      method_descriptor"
     ]
    }
   ],
   "source": [
    "print(text.split())\n",
    "print(text.split(\"?\"))\n",
    "\n",
    "str.split? # same as help(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62356eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fed47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ')', '.', ':', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!:)]|\\s)', text)\n",
    "tokens = [item for item in tokens if item.strip()]\n",
    "tokens = sorted(list(set(tokens)))  # unique tokens only\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f014f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ')',\n",
       " '.',\n",
       " ':',\n",
       " '?',\n",
       " 'Or',\n",
       " 'Test',\n",
       " 'This',\n",
       " 'a',\n",
       " 'be',\n",
       " 'is',\n",
       " 'it',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'test',\n",
       " 'this',\n",
       " 'to']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71796eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('!', 0), (')', 1), ('.', 2), (':', 3), ('?', 4), ('Or', 5), ('Test', 6), ('This', 7), ('a', 8), ('be', 9), ('is', 10), ('it', 11), ('not', 12), ('sure', 13), ('test', 14), ('this', 15), ('to', 16)])\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:index for index, token in enumerate(tokens)} # dictionary comprehension\n",
    "print(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f950571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09addc",
   "metadata": {},
   "source": [
    "# Tokenizing a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63f6047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"thousand_and_second_night.txt\", \"r\") as f:\n",
    "    tokens = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d1f2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ')', '.', ':', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!:)]|\\s)', text)\n",
    "tokens = [item for item in tokens if item.strip()]\n",
    "tokens = sorted(list(set(tokens)))  # unique tokens only\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
